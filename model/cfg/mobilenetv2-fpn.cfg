[net]
channels=3

# conv1
[convolutional]
filters=32
size=3
pad=1
stride=2
batch_normalize=1
activation=relu6

# conv2_1_dwise
[convolutional]
groups=32
filters=32
size=3
stride=1
pad=1
batch_normalize=1
activation=relu6

# conv2_1_linear
[convolutional]
filters=16
size=1
stride=1
pad=1
batch_normalize=1
activation=linear

# conv2_2_expand
[convolutional]
filters=96
size=1
stride=1
pad=1
batch_normalize=1
activation=relu6

# conv2_2_dwise
[convolutional]
groups=96
filters=96
size=3
pad=1
stride=2
batch_normalize=1
activation=relu6

# conv2_2_linear
[convolutional]
filters=24
size=1
stride=1
pad=1
batch_normalize=1
activation=linear

# conv3_1_expand
[convolutional]
filters=144
size=1
stride=1
pad=1
batch_normalize=1
activation=relu6

# conv3_1_dwise
[convolutional]
groups=144
filters=144
size=3
stride=1
pad=1
batch_normalize=1
activation=relu6

# conv3_1_linear
[convolutional]
filters=24
size=1
stride=1
pad=1
batch_normalize=1
activation=linear

# block_3_1
[shortcut]
from=-4
activation=linear

# conv_3_2_expand
[convolutional]
filters=144
size=1
stride=1
pad=1
batch_normalize=1
activation=relu6

# conv_3_2_dwise
[convolutional]
groups=144
filters=144
size=3
pad=1
stride=2
batch_normalize=1
activation=relu6

# conv_3_2_linear
[convolutional]
filters=32
size=1
stride=1
pad=1
batch_normalize=1
activation=linear

# conv_4_1_expand
[convolutional]
filters=192
size=1
stride=1
pad=1
batch_normalize=1
activation=relu6

# conv_4_1_dwise
[convolutional]
groups=192
filters=192
size=3
stride=1
pad=1
batch_normalize=1
activation=relu6

# conv_4_1_linear
[convolutional]
filters=32
size=1
stride=1
pad=1
batch_normalize=1
activation=linear

# block_4_1
[shortcut]
from=-4
activation=linear

# conv_4_2_expand
[convolutional]
filters=192
size=1
stride=1
pad=1
batch_normalize=1
activation=relu6

# conv_4_2_dwise
[convolutional]
groups=192
filters=192
size=3
stride=1
pad=1
batch_normalize=1
activation=relu6

# conv_4_2_linear
[convolutional]
filters=32
size=1
stride=1
pad=1
batch_normalize=1
activation=linear

# block_4_2
[shortcut]
from=-4
activation=linear

# conv_4_3_expand
[convolutional]
filters=192
size=1
stride=1
pad=1
batch_normalize=1
activation=relu6

# conv_4_3_dwise
[convolutional]
groups=192
filters=192
size=3
stride=2
pad=1
batch_normalize=1
activation=relu6

# conv_4_3_linear
[convolutional]
filters=64
size=1
stride=1
pad=1
batch_normalize=1
activation=linear

# conv_4_4_expand
[convolutional]
filters=384
size=1
stride=1
pad=1
batch_normalize=1
activation=relu6

# conv_4_4_dwise
[convolutional]
groups=384
filters=384
size=3
stride=1
pad=1
batch_normalize=1
activation=relu6

# conv_4_4_linear
[convolutional]
filters=64
size=1
stride=1
pad=1
batch_normalize=1
activation=linear

# block_4_4
[shortcut]
from=-4
activation=linear

# conv_4_5_expand
[convolutional]
filters=384
size=1
stride=1
pad=1
batch_normalize=1
activation=relu6

# conv_4_5_dwise
[convolutional]
groups=384
filters=384
size=3
stride=1
pad=1
batch_normalize=1
activation=relu6

# conv_4_5_linear
[convolutional]
filters=64
size=1
stride=1
pad=1
batch_normalize=1
activation=linear

# block_4_5
[shortcut]
from=-4
activation=linear

# conv_4_6_expand
[convolutional]
filters=384
size=1
stride=1
pad=1
batch_normalize=1
activation=relu6

# conv_4_6_dwise
[convolutional]
groups=384
filters=384
size=3
stride=1
pad=1
batch_normalize=1
activation=relu6

# conv_4_6_linear
[convolutional]
filters=64
size=1
stride=1
pad=1
batch_normalize=1
activation=linear

# block_4_6
[shortcut]
from=-4
activation=linear

# conv_4_7_expand
[convolutional]
filters=384
size=1
stride=1
pad=1
batch_normalize=1
activation=relu6

# conv_4_7_dwise
[convolutional]
groups=384
filters=384
size=3
pad=1
stride=1
batch_normalize=1
activation=relu6

# conv_4_7_linear
[convolutional]
filters=96
size=1
stride=1
pad=1
batch_normalize=1
activation=linear

# conv_5_1_expand
[convolutional]
filters=576
size=1
stride=1
pad=1
batch_normalize=1
activation=relu6

# conv_5_1_dwise
[convolutional]
groups=576
filters=576
size=3
stride=1
pad=1
batch_normalize=1
activation=relu6

# conv_5_1_linear
[convolutional]
filters=96
size=1
stride=1
pad=1
batch_normalize=1
activation=linear

# block_5_1
[shortcut]
from=-4
activation=linear

# conv_5_2_expand
[convolutional]
filters=576
size=1
stride=1
pad=1
batch_normalize=1
activation=relu6

# conv_5_2_dwise
[convolutional]
groups=576
filters=576
size=3
stride=1
pad=1
batch_normalize=1
activation=relu6

# conv_5_2_linear
[convolutional]
filters=96
size=1
stride=1
pad=1
batch_normalize=1
activation=linear

# block_5_2
[shortcut]
from=-4
activation=linear

# conv_5_3_expand
[convolutional]
filters=576
size=1
stride=1
pad=1
batch_normalize=1
activation=relu6

# conv_5_3_dwise
[convolutional]
groups=576
filters=576
size=3
pad=1
stride=2
batch_normalize=1
activation=relu6

# conv_5_3_linear
[convolutional]
filters=160
size=1
stride=1
pad=1
batch_normalize=1
activation=linear

# conv_6_1_expand
[convolutional]
filters=960
size=1
stride=1
pad=1
batch_normalize=1
activation=relu6

# conv_6_1_dwise
[convolutional]
groups=960
filters=960
size=3
stride=1
pad=1
batch_normalize=1
activation=relu6

# conv_6_1_linear
[convolutional]
filters=160
size=1
stride=1
pad=1
batch_normalize=1
activation=linear

# block_6_1
[shortcut]
from=-4
activation=linear

# conv_6_2_expand
[convolutional]
filters=960
size=1
stride=1
pad=1
batch_normalize=1
activation=relu6

# conv_6_2_dwise
[convolutional]
groups=960
filters=960
size=3
stride=1
pad=1
batch_normalize=1
activation=relu6

# conv_6_2_linear
[convolutional]
filters=160
size=1
stride=1
pad=1
batch_normalize=1
activation=linear

# block_6_2
[shortcut]
from=-4
activation=linear

# conv_6_3_expand
[convolutional]
filters=960
size=1
stride=1
pad=1
batch_normalize=1
activation=relu6

# conv_6_3_dwise
[convolutional]
groups=960
filters=960
size=3
stride=1
pad=1
batch_normalize=1
activation=relu6

# conv_6_3_linear
[convolutional]
filters=320
size=1
stride=1
pad=1
batch_normalize=1
activation=linear

# conv_6_4
[convolutional]
filters=1280
size=1
stride=1
pad=1
batch_normalize=1
activation=relu6

#########################

# detect large
[convolutional]
filters=512
size=1
stride=1
pad=1
batch_normalize=1
activation=relu6

[convolutional]
filters=512
groups=512
size=3
stride=1
pad=1
batch_normalize=1
activation=relu6

[convolutional]
filters=1024
size=1
stride=1
pad=1
batch_normalize=1
activation=relu6

[convolutional]
filters=512
size=1
stride=1
pad=1
batch_normalize=1
activation=relu6

[convolutional]
filters=512
groups=512
size=3
stride=1
pad=1
batch_normalize=1
activation=relu6

[convolutional]
filters=1024
size=1
stride=1
pad=1
batch_normalize=1
activation=relu6

[convolutional]
filters=512
size=1
stride=1
pad=1
batch_normalize=1
activation=relu6

[convolutional]
filters=512
groups=512
size=3
stride=1
pad=1
batch_normalize=1
activation=relu6

[convolutional]
filters=1024
size=1
stride=1
pad=1
batch_normalize=1
activation=relu6

[convolutional]
filters=75
size=1
stride=1
pad=1
activation=linear

[yolo]
classes=20
ignore_thresh = .5
bbox_loss=l1
l1_loss_gain=0.05

# merge to middle
[route]
layers = -5

[convolutional]
filters=256
size=1
stride=1
pad=1
batch_normalize=1
activation=relu6

[upsample]
stride=2

[route]
layers = -1, 46

# detect middle
[convolutional]
filters=256
size=1
stride=1
pad=1
batch_normalize=1
activation=relu6

[convolutional]
filters=256
groups=256
size=3
stride=1
pad=1
batch_normalize=1
activation=relu6

[convolutional]
filters=512
size=1
stride=1
pad=1
batch_normalize=1
activation=relu6

[convolutional]
filters=256
size=1
stride=1
pad=1
batch_normalize=1
activation=relu6

[convolutional]
filters=256
groups=256
size=3
stride=1
pad=1
batch_normalize=1
activation=relu6

[convolutional]
filters=512
size=1
stride=1
pad=1
batch_normalize=1
activation=relu6

[convolutional]
filters=256
size=1
stride=1
pad=1
batch_normalize=1
activation=relu6

[convolutional]
filters=256
groups=256
size=3
stride=1
pad=1
batch_normalize=1
activation=relu6

[convolutional]
filters=512
size=1
stride=1
pad=1
batch_normalize=1
activation=relu6

[convolutional]
filters=75
size=1
stride=1
pad=1
activation=linear

[yolo]
classes=20
ignore_thresh = .5
bbox_loss=l1
l1_loss_gain=0.05

# merge to middle
[route]
layers = -5

[convolutional]
filters=128
size=1
stride=1
pad=1
batch_normalize=1
activation=relu6

[upsample]
stride=2

[route]
layers = -1, 20

# detect small
[convolutional]
filters=128
size=1
stride=1
pad=1
batch_normalize=1
activation=relu6

[convolutional]
filters=128
groups=128
size=3
stride=1
pad=1
batch_normalize=1
activation=relu6

[convolutional]
filters=256
size=1
stride=1
pad=1
batch_normalize=1
activation=relu6

[convolutional]
filters=128
size=1
stride=1
pad=1
batch_normalize=1
activation=relu6

[convolutional]
filters=128
groups=128
size=3
stride=1
pad=1
batch_normalize=1
activation=relu6

[convolutional]
filters=256
size=1
stride=1
pad=1
batch_normalize=1
activation=relu6

[convolutional]
filters=128
size=1
stride=1
pad=1
batch_normalize=1
activation=relu6

[convolutional]
filters=128
groups=128
size=3
stride=1
pad=1
batch_normalize=1
activation=relu6

[convolutional]
filters=256
size=1
stride=1
pad=1
batch_normalize=1
activation=relu6

[convolutional]
filters=75
size=1
stride=1
pad=1
activation=linear

[yolo]
classes=20
ignore_thresh = .5
bbox_loss=l1
l1_loss_gain=0.05