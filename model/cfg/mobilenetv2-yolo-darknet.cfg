[net]
# Testing
# batch=1
# subdivisions=1
# Training
batch=64
subdivisions=16
width=416
height=416
channels=3
momentum=0.9
decay=0.0005
angle=0
saturation = 1.5
exposure = 1.5
hue=.1

learning_rate=0.001
burn_in=1000
max_batches = 500200
policy=steps
steps=400000,450000
scales=.1,.1

# conv1
[convolutional]
filters=32
size=3
pad=1
stride=2
batch_normalize=1
activation=relu

# conv2_1_dwise
[convolutional]
groups=32
filters=32
size=3
stride=1
pad=1
batch_normalize=1
activation=relu

# conv2_1_linear
[convolutional]
filters=16
size=1
stride=1
pad=1
batch_normalize=1
activation=linear

# conv2_2_expand
[convolutional]
filters=96
size=1
stride=1
pad=1
batch_normalize=1
activation=relu

# conv2_2_dwise
[convolutional]
groups=96
filters=96
size=3
pad=1
stride=2
batch_normalize=1
activation=relu

# conv2_2_linear
[convolutional]
filters=24
size=1
stride=1
pad=1
batch_normalize=1
activation=linear

# conv3_1_expand
[convolutional]
filters=144
size=1
stride=1
pad=1
batch_normalize=1
activation=relu

# conv3_1_dwise
[convolutional]
groups=144
filters=144
size=3
stride=1
pad=1
batch_normalize=1
activation=relu

# conv3_1_linear
[convolutional]
filters=24
size=1
stride=1
pad=1
batch_normalize=1
activation=linear

# block_3_1
[shortcut]
from=-4
activation=linear

# conv_3_2_expand
[convolutional]
filters=144
size=1
stride=1
pad=1
batch_normalize=1
activation=relu

# conv_3_2_dwise
[convolutional]
groups=144
filters=144
size=3
pad=1
stride=2
batch_normalize=1
activation=relu

# conv_3_2_linear
[convolutional]
filters=32
size=1
stride=1
pad=1
batch_normalize=1
activation=linear

# conv_4_1_expand
[convolutional]
filters=192
size=1
stride=1
pad=1
batch_normalize=1
activation=relu

# conv_4_1_dwise
[convolutional]
groups=192
filters=192
size=3
stride=1
pad=1
batch_normalize=1
activation=relu

# conv_4_1_linear
[convolutional]
filters=32
size=1
stride=1
pad=1
batch_normalize=1
activation=linear

# block_4_1
[shortcut]
from=-4
activation=linear

# conv_4_2_expand
[convolutional]
filters=192
size=1
stride=1
pad=1
batch_normalize=1
activation=relu

# conv_4_2_dwise
[convolutional]
groups=192
filters=192
size=3
stride=1
pad=1
batch_normalize=1
activation=relu

# conv_4_2_linear
[convolutional]
filters=32
size=1
stride=1
pad=1
batch_normalize=1
activation=linear

# block_4_2
[shortcut]
from=-4
activation=linear

# conv_4_3_expand
[convolutional]
filters=192
size=1
stride=1
pad=1
batch_normalize=1
activation=relu

# conv_4_3_dwise
[convolutional]
groups=192
filters=192
size=3
stride=2
pad=1
batch_normalize=1
activation=relu

# conv_4_3_linear
[convolutional]
filters=64
size=1
stride=1
pad=1
batch_normalize=1
activation=linear

# conv_4_4_expand
[convolutional]
filters=384
size=1
stride=1
pad=1
batch_normalize=1
activation=relu

# conv_4_4_dwise
[convolutional]
groups=384
filters=384
size=3
stride=1
pad=1
batch_normalize=1
activation=relu

# conv_4_4_linear
[convolutional]
filters=64
size=1
stride=1
pad=1
batch_normalize=1
activation=linear

# block_4_4
[shortcut]
from=-4
activation=linear

# conv_4_5_expand
[convolutional]
filters=384
size=1
stride=1
pad=1
batch_normalize=1
activation=relu

# conv_4_5_dwise
[convolutional]
groups=384
filters=384
size=3
stride=1
pad=1
batch_normalize=1
activation=relu

# conv_4_5_linear
[convolutional]
filters=64
size=1
stride=1
pad=1
batch_normalize=1
activation=linear

# block_4_5
[shortcut]
from=-4
activation=linear

# conv_4_6_expand
[convolutional]
filters=384
size=1
stride=1
pad=1
batch_normalize=1
activation=relu

# conv_4_6_dwise
[convolutional]
groups=384
filters=384
size=3
stride=1
pad=1
batch_normalize=1
activation=relu

# conv_4_6_linear
[convolutional]
filters=64
size=1
stride=1
pad=1
batch_normalize=1
activation=linear

# block_4_6
[shortcut]
from=-4
activation=linear

# conv_4_7_expand
[convolutional]
filters=384
size=1
stride=1
pad=1
batch_normalize=1
activation=relu

# conv_4_7_dwise
[convolutional]
groups=384
filters=384
size=3
pad=1
stride=1
batch_normalize=1
activation=relu

# conv_4_7_linear
[convolutional]
filters=96
size=1
stride=1
pad=1
batch_normalize=1
activation=linear

# conv_5_1_expand
[convolutional]
filters=576
size=1
stride=1
pad=1
batch_normalize=1
activation=relu

# conv_5_1_dwise
[convolutional]
groups=576
filters=576
size=3
stride=1
pad=1
batch_normalize=1
activation=relu

# conv_5_1_linear
[convolutional]
filters=96
size=1
stride=1
pad=1
batch_normalize=1
activation=linear

# block_5_1
[shortcut]
from=-4
activation=linear

# conv_5_2_expand
[convolutional]
filters=576
size=1
stride=1
pad=1
batch_normalize=1
activation=relu

# conv_5_2_dwise
[convolutional]
groups=576
filters=576
size=3
stride=1
pad=1
batch_normalize=1
activation=relu

# conv_5_2_linear
[convolutional]
filters=96
size=1
stride=1
pad=1
batch_normalize=1
activation=linear

# block_5_2
[shortcut]
from=-4
activation=linear

# conv_5_3_expand
[convolutional]
filters=576
size=1
stride=1
pad=1
batch_normalize=1
activation=relu

# conv_5_3_dwise
[convolutional]
groups=576
filters=576
size=3
pad=1
stride=2
batch_normalize=1
activation=relu

# conv_5_3_linear
[convolutional]
filters=160
size=1
stride=1
pad=1
batch_normalize=1
activation=linear

# conv_6_1_expand
[convolutional]
filters=960
size=1
stride=1
pad=1
batch_normalize=1
activation=relu

# conv_6_1_dwise
[convolutional]
groups=960
filters=960
size=3
stride=1
pad=1
batch_normalize=1
activation=relu

# conv_6_1_linear
[convolutional]
filters=160
size=1
stride=1
pad=1
batch_normalize=1
activation=linear

# block_6_1
[shortcut]
from=-4
activation=linear

# conv_6_2_expand
[convolutional]
filters=960
size=1
stride=1
pad=1
batch_normalize=1
activation=relu

# conv_6_2_dwise
[convolutional]
groups=960
filters=960
size=3
stride=1
pad=1
batch_normalize=1
activation=relu

# conv_6_2_linear
[convolutional]
filters=160
size=1
stride=1
pad=1
batch_normalize=1
activation=linear

# block_6_2
[shortcut]
from=-4
activation=linear

# conv_6_3_expand
[convolutional]
filters=960
size=1
stride=1
pad=1
batch_normalize=1
activation=relu

# conv_6_3_dwise
[convolutional]
groups=960
filters=960
size=3
stride=1
pad=1
batch_normalize=1
activation=relu

# conv_6_3_linear
[convolutional]
filters=320
size=1
stride=1
pad=1
batch_normalize=1
activation=linear

# conv_6_4
[convolutional]
filters=1280
size=1
stride=1
pad=1
batch_normalize=1
activation=relu

#########################

# detect large
[convolutional]
filters=512
size=1
stride=1
pad=1
batch_normalize=1
activation=relu

[convolutional]
filters=512
groups=512
size=3
stride=1
pad=1
batch_normalize=1
activation=relu

[convolutional]
filters=1024
size=1
stride=1
pad=1
batch_normalize=1
activation=relu

[convolutional]
filters=512
size=1
stride=1
pad=1
batch_normalize=1
activation=relu

[convolutional]
filters=512
groups=512
size=3
stride=1
pad=1
batch_normalize=1
activation=relu

[convolutional]
filters=1024
size=1
stride=1
pad=1
batch_normalize=1
activation=relu

[convolutional]
filters=512
size=1
stride=1
pad=1
batch_normalize=1
activation=relu

[convolutional]
filters=512
groups=512
size=3
stride=1
pad=1
batch_normalize=1
activation=relu

[convolutional]
filters=1024
size=1
stride=1
pad=1
batch_normalize=1
activation=relu

[convolutional]
filters=75
size=1
stride=1
pad=1
activation=linear

[yolo]
mask = 6,7,8
anchors = 0,0,  0,0,  0,0,  0,0,  0,0,  0,0,  0,0,  0,0,  0,0
classes=20
num=9
jitter=.3
ignore_thresh = .5
truth_thresh = 1
random=1

# merge to middle
[route]
layers = -5

[convolutional]
filters=256
size=1
stride=1
pad=1
batch_normalize=1
activation=relu

[upsample]
stride=2

[route]
layers = -1, 46

# detect middle
[convolutional]
filters=256
size=1
stride=1
pad=1
batch_normalize=1
activation=relu

[convolutional]
filters=256
groups=256
size=3
stride=1
pad=1
batch_normalize=1
activation=relu

[convolutional]
filters=512
size=1
stride=1
pad=1
batch_normalize=1
activation=relu

[convolutional]
filters=256
size=1
stride=1
pad=1
batch_normalize=1
activation=relu

[convolutional]
filters=256
groups=256
size=3
stride=1
pad=1
batch_normalize=1
activation=relu

[convolutional]
filters=512
size=1
stride=1
pad=1
batch_normalize=1
activation=relu

[convolutional]
filters=256
size=1
stride=1
pad=1
batch_normalize=1
activation=relu

[convolutional]
filters=256
groups=256
size=3
stride=1
pad=1
batch_normalize=1
activation=relu

[convolutional]
filters=512
size=1
stride=1
pad=1
batch_normalize=1
activation=relu

[convolutional]
filters=75
size=1
stride=1
pad=1
activation=linear

[yolo]
mask = 3,4,5
anchors = 0,0,  0,0,  0,0,  0,0,  0,0,  0,0,  0,0,  0,0,  0,0
classes=20
num=9
jitter=.3
ignore_thresh = .5
truth_thresh = 1
random=1

# merge to middle
[route]
layers = -5

[convolutional]
filters=128
size=1
stride=1
pad=1
batch_normalize=1
activation=relu

[upsample]
stride=2

[route]
layers = -1, 20

# detect small
[convolutional]
filters=128
size=1
stride=1
pad=1
batch_normalize=1
activation=relu

[convolutional]
filters=128
groups=128
size=3
stride=1
pad=1
batch_normalize=1
activation=relu

[convolutional]
filters=256
size=1
stride=1
pad=1
batch_normalize=1
activation=relu

[convolutional]
filters=128
size=1
stride=1
pad=1
batch_normalize=1
activation=relu

[convolutional]
filters=128
groups=128
size=3
stride=1
pad=1
batch_normalize=1
activation=relu

[convolutional]
filters=256
size=1
stride=1
pad=1
batch_normalize=1
activation=relu

[convolutional]
filters=128
size=1
stride=1
pad=1
batch_normalize=1
activation=relu

[convolutional]
filters=128
groups=128
size=3
stride=1
pad=1
batch_normalize=1
activation=relu

[convolutional]
filters=256
size=1
stride=1
pad=1
batch_normalize=1
activation=relu

[convolutional]
filters=75
size=1
stride=1
pad=1
activation=linear

[yolo]
mask = 0,1,2
anchors = 0,0,  0,0,  0,0,  0,0,  0,0,  0,0,  0,0,  0,0,  0,0
classes=20
num=9
jitter=.3
ignore_thresh = .5
truth_thresh = 1
random=1